{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras.optimizers as optimizers\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from models.classification_model import ClassificationDGP\n",
    "from likelihoods import Softmax\n",
    "from utils_dataset import load_tf_dataset\n",
    "from utils_training import MCEM_sampler_classification, MCEM_Q_maximizer, MCEM, MCEM_windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############################## Getting data info:dataset name: boston ##############################\n",
      "D: 13, N: 455, Ns: 51\n",
      "X_mean: [1.7378345e+00 1.1138461e+01 1.1224440e+01 7.2527476e-02 5.5657738e-01\n",
      " 6.2923098e+00 6.9054504e+01 3.6594815e+00 4.3472528e+00 4.0854724e+02\n",
      " 1.8481098e+01 3.5551254e+02 1.2646418e+01], Y_mean: [22.656263], Y_std: [9.32293]\n",
      "######################################################################\n",
      "Dataset boston, total size:506\n"
     ]
    }
   ],
   "source": [
    "# dataset information loading\n",
    "dataset_name = 'mnist'\n",
    "_, _, train_full_size, test_full_size = load_tf_dataset(dataset_name, data_dir='./tensorflow_datasets/')\n",
    "dataset_size = train_full_size + test_full_size\n",
    "print(f\"Dataset {dataset_name}, total size:{dataset_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2-layer DGPs with Kernel type RBF\n"
     ]
    }
   ],
   "source": [
    "# model setting\n",
    "d_in = 28 * 28\n",
    "d_out = 10\n",
    "n_gp = 30\n",
    "model = ClassificationDGP(d_in, d_out, n_hidden_layers=2, n_rf=100, n_gp=n_gp,\n",
    "                      likelihood=Softmax(),\n",
    "                      kernel_type_list=['RBF','RBF'], kernel_trainable=True,\n",
    "                      random_fixed=True, input_cat=True)\n",
    "print(f\"{model.n_hidden_layers}-layer DGPs with Kernel type {model.kernel_type_list[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############################## Getting data info:dataset name: boston ##############################\n",
      "D: 13, N: 455, Ns: 51\n",
      "X_mean: [1.7378345e+00 1.1138461e+01 1.1224440e+01 7.2527476e-02 5.5657738e-01\n",
      " 6.2923098e+00 6.9054504e+01 3.6594815e+00 4.3472528e+00 4.0854724e+02\n",
      " 1.8481098e+01 3.5551254e+02 1.2646418e+01], Y_mean: [22.656263], Y_std: [9.32293]\n",
      "######################################################################\n",
      "Training size is 400 after remainder dropping. \n",
      "############################## Getting data info:dataset name: boston ##############################\n",
      "D: 13, N: 455, Ns: 51\n",
      "X_mean: [1.7378345e+00 1.1138461e+01 1.1224440e+01 7.2527476e-02 5.5657738e-01\n",
      " 6.2923098e+00 6.9054504e+01 3.6594815e+00 4.3472528e+00 4.0854724e+02\n",
      " 1.8481098e+01 3.5551254e+02 1.2646418e+01], Y_mean: [22.656263], Y_std: [9.32293]\n",
      "######################################################################\n",
      "Training size is 400 after remainder dropping. \n"
     ]
    }
   ],
   "source": [
    "# EM sampler settings\n",
    "batch_size= 200\n",
    "lr_mcmc_0 = 0.01\n",
    "beta_mcmc = 0.0\n",
    "sampler_EM = MCEM_sampler_classification(model, dataset_name=dataset_name, batch_size=batch_size,\n",
    "                                         lr_0=lr_mcmc_0, momentum_decay=beta_mcmc,\n",
    "                                         precond_type='identity', K_batches=None, second_moment_centered=None,\n",
    "                                         resample_in_cycle_head=False, start_sampling_epoch=2000, epochs_per_cycle=50)\n",
    "# Maximizer setttings\n",
    "lr_maximizer = 0.01\n",
    "optimizer = optimizers.Adam(learning_rate=lr_maximizer)\n",
    "maximizer = MCEM_Q_maximizer(model, dataset_size, optimizer)\n",
    "# sampler settings after fixing hyper-params\n",
    "lr_fixing_hyper_0 = 0.01\n",
    "beta_fixing = 0.0\n",
    "sampler_fixing_hyper = MCEM_sampler_classification(model, dataset_name=dataset_name, batch_size=batch_size,\n",
    "                                                   lr_0=lr_fixing_hyper_0, momentum_decay=beta_fixing,\n",
    "                                                   precond_type='identity', K_batches=None, second_moment_centered=None,\n",
    "                                                   resample_in_cycle_head=False,\n",
    "                                                   start_sampling_epoch=2000, epochs_per_cycle=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# MCEM settings and training\n",
    "total_EM_steps = 20000\n",
    "ds_train_M, _, _, _ = load_tf_dataset(dataset_name, batch_size=100)\n",
    "log_p, acc = MCEM(sampler_EM, maximizer, sampler_fixing_hyper, total_EM_steps, ds_train_M,\n",
    "                  num_samples_EM=100, num_samples_fixing_hyper=200,\n",
    "                  print_epoch_cycle_EM=200, print_epoch_cycle_fixing=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "drop_mixing = 0\n",
    "log_p_droped = log_p[drop_mixing:, :]\n",
    "mse_droped = mse[drop_mixing:, :]\n",
    "\n",
    "n_models_droped = tf.shape(mse_droped)[0]\n",
    "predict_log_p = tf.reduce_logsumexp(log_p_droped, axis=0) - tf.math.log(tf.cast(n_models_droped, tf.float32))\n",
    "predict_log_p = tf.reduce_mean(predict_log_p)\n",
    "predict_acc = tf.reduce_mean(mse_droped)\n",
    "\n",
    "print(f\"Dataset: {dataset_name}\")\n",
    "print(f\"Number of sampled models(after dropping {drop_mixing} samples): {n_models_droped}\")\n",
    "print(f\"Test Log Likelihood of all sampled models: {predict_log_p}\")\n",
    "print(f\"Test Root MSE of all sampled models: {predict_rmse}\")\n",
    "\n",
    "log_p_coll.append(predict_log_p)\n",
    "rmse_coll.append(predict_rmse)\n",
    "print('*' * 70)\n",
    "print('*' * 70)\n",
    "print(' ')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}