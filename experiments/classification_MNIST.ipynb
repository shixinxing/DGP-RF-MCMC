{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from models.classification_model import ClassificationDGP\n",
    "from likelihoods import Softmax\n",
    "from utils_training import classification_train\n",
    "# setting dataset\n",
    "record_file = 'MNIST_results.txt'\n",
    "repeat = 3 # run the task several times\n",
    "print('#'*40, f\"Results stored in {record_file}.\", '#'*40)\n",
    "\n",
    "dataset_name_list = ['mnist']\n",
    "    # , 'wine_red', 'wine_white', 'concrete',\n",
    "    #                  'energy', 'kin8nm', 'naval', 'power','protein']\n",
    "d_in_list = [28*28]\n",
    "    # , 11, 11, 8, 8, 8, 16, 4, 9]\n",
    "d_out = 10\n",
    "# full Bayesian training settings\n",
    "batch_size = 200\n",
    "lr_0 = 0.1\n",
    "beta = 0.0\n",
    "total_epochs = 2000\n",
    "start_sampling_epoch = 150\n",
    "epochs_per_cycle = 50\n",
    "print_epoch_cycle = 50\n",
    "drop_mixing = 17\n",
    "for i in range(len(dataset_name_list)):\n",
    "    dataset_name = dataset_name_list[i]\n",
    "    hid_n_gp = 30\n",
    "\n",
    "    log_p_coll = []\n",
    "    acc_coll = []\n",
    "    for run_index in range(repeat):\n",
    "        print('*' * 70)\n",
    "        print('*' * 30, '2-layer DGPs ', f'Run {run_index}', '*' * 30)\n",
    "        model = ClassificationDGP(d_in_list[i], d_out, n_hidden_layers=2, n_rf=100,\n",
    "                                  n_gp=[hid_n_gp, d_out], likelihood=Softmax(),\n",
    "                                  kernel_type_list=['RBF', 'RBF'], kernel_trainable=True,\n",
    "                                  random_fixed=True, set_nonzero_mean=False, input_cat=True)\n",
    "        # return matrix [S, N]\n",
    "        log_p, acc = classification_train(model, dataset_name=dataset_name, batch_size=batch_size,\n",
    "                                          lr_0=lr_0, momentum_decay=beta, full_bayesian=True,\n",
    "                                          precond_type='identity',\n",
    "                                          resample_in_cycle_head=False,\n",
    "                                          total_epochs=total_epochs, start_sampling_epoch=start_sampling_epoch,\n",
    "                                          epochs_per_cycle=epochs_per_cycle,\n",
    "                                          print_epoch_cycle=print_epoch_cycle)\n",
    "        log_p_droped = log_p[drop_mixing:, :]\n",
    "        acc_droped = acc[drop_mixing:]\n",
    "\n",
    "        n_models_droped = tf.shape(acc_droped)[0]\n",
    "        predict_log_p = tf.reduce_logsumexp(log_p_droped, axis=0) - tf.math.log(tf.cast(n_models_droped, tf.float32))\n",
    "        predict_log_p = tf.reduce_mean(predict_log_p)\n",
    "        predict_acc = tf.reduce_mean(acc_droped)\n",
    "\n",
    "        print(f\"Dataset: {dataset_name}, Full Bayes \")\n",
    "        print(f\"Number of sampled models(after dropping {drop_mixing} samples): {n_models_droped}\")\n",
    "        print(f\"Test Log Likelihood of all sampled models: {predict_log_p}\")\n",
    "        print(f\"Test Root MSE of all sampled models: {predict_acc}\")\n",
    "\n",
    "        log_p_coll.append(predict_log_p)\n",
    "        acc_coll.append(predict_acc)\n",
    "        print('*' * 70)\n",
    "        print('*' * 70, \"\\n\")\n",
    "\n",
    "    log_p_coll = tf.concat(log_p_coll, axis=0)\n",
    "    acc_coll = tf.concat(acc_coll, axis=0)\n",
    "    with open(record_file, 'a') as f:\n",
    "        f.write(f\"Dataset: {dataset_name}, with no preconditioner\\n\")\n",
    "        f.write(f\"Predict Mean Log Likelihood: {log_p_coll},\\n  \")\n",
    "        f.write(f\"Their mean is: {tf.reduce_mean(log_p_coll)}, \")\n",
    "        f.write(f\"std is: {tf.math.reduce_std(log_p_coll)}\\n\")\n",
    "        f.write(f\"Predict Mean Acc: {acc_coll},\\n  \")\n",
    "        f.write(f\"Their mean is: {tf.reduce_mean(acc_coll)}, \")\n",
    "        f.write(f\"std is: {tf.math.reduce_std(acc_coll)}\\n\\n\")\n",
    "# with rmsprop preconditioner\n",
    "# full Bayesian training settings\n",
    "batch_size = 200\n",
    "lr_0 = 0.1\n",
    "beta = 0.0\n",
    "total_epochs = 2000\n",
    "start_sampling_epoch = 150\n",
    "epochs_per_cycle = 50\n",
    "print_epoch_cycle = 25\n",
    "drop_mixing = 17\n",
    "for i in range(len(dataset_name_list)):\n",
    "    dataset_name = dataset_name_list[i]\n",
    "    hid_n_gp = 30\n",
    "\n",
    "    log_p_coll = []\n",
    "    acc_coll = []\n",
    "    for run_index in range(repeat):\n",
    "        print('*' * 70)\n",
    "        print('*' * 30, '2-layer DGPs ', f'Run {run_index}', '*' * 30)\n",
    "        model = ClassificationDGP(d_in_list[i], d_out, n_hidden_layers=2, n_rf=100,\n",
    "                                  n_gp=[hid_n_gp, d_out], likelihood=Softmax(),\n",
    "                                  kernel_type_list=['RBF', 'RBF'], kernel_trainable=True,\n",
    "                                  random_fixed=True, set_nonzero_mean=False, input_cat=True)\n",
    "        # return matrix [S, N]\n",
    "        log_p, acc = classification_train(model, dataset_name=dataset_name, batch_size=batch_size,\n",
    "                                          lr_0=lr_0, momentum_decay=beta, full_bayesian=True,\n",
    "                                          precond_type='rmsprop', K_batches=32,second_moment_centered=False,\n",
    "                                          resample_in_cycle_head=False,\n",
    "                                          total_epochs=total_epochs, start_sampling_epoch=start_sampling_epoch,\n",
    "                                          epochs_per_cycle=epochs_per_cycle,\n",
    "                                          print_epoch_cycle=print_epoch_cycle)\n",
    "        log_p_droped = log_p[drop_mixing:, :]\n",
    "        acc_droped = acc[drop_mixing:]\n",
    "\n",
    "        n_models_droped = tf.shape(acc_droped)[0]\n",
    "        predict_log_p = tf.reduce_logsumexp(log_p_droped, axis=0) - tf.math.log(tf.cast(n_models_droped, tf.float32))\n",
    "        predict_log_p = tf.reduce_mean(predict_log_p)\n",
    "        predict_acc = tf.reduce_mean(acc_droped)\n",
    "\n",
    "        print(f\"Dataset: {dataset_name}, Full Bayes \")\n",
    "        print(f\"Number of sampled models(after dropping {drop_mixing} samples): {n_models_droped}\")\n",
    "        print(f\"Test Log Likelihood of all sampled models: {predict_log_p}\")\n",
    "        print(f\"Test Root MSE of all sampled models: {predict_acc}\")\n",
    "\n",
    "        log_p_coll.append(predict_log_p)\n",
    "        acc_coll.append(predict_acc)\n",
    "        print('*' * 70)\n",
    "        print('*' * 70, \"\\n\")\n",
    "\n",
    "    log_p_coll = tf.concat(log_p_coll, axis=0)\n",
    "    acc_coll = tf.concat(acc_coll, axis=0)\n",
    "    with open(record_file, 'a') as f:\n",
    "        f.write(f\"Dataset: {dataset_name}, with RMSProp preconditioner \\n\")\n",
    "        f.write(f\"Predict Mean Log Likelihood: {log_p_coll},\\n  \")\n",
    "        f.write(f\"Their mean is: {tf.reduce_mean(log_p_coll)}, \")\n",
    "        f.write(f\"std is: {tf.math.reduce_std(log_p_coll)}\\n\")\n",
    "        f.write(f\"Predict Mean Acc: {acc_coll},\\n  \")\n",
    "        f.write(f\"Their mean is: {tf.reduce_mean(acc_coll)}, \")\n",
    "        f.write(f\"std is: {tf.math.reduce_std(acc_coll)}\\n\\n\")\n",
    "# fashion mnist\n",
    "# setting dataset\n",
    "record_file = 'Fashion_MNIST_results.txt'\n",
    "repeat = 1 # run the task several times\n",
    "print('#'*40, f\"Results stored in {record_file}.\", '#'*40)\n",
    "\n",
    "dataset_name_list = ['fashion_mnist']\n",
    "    # , 'wine_red', 'wine_white', 'concrete',\n",
    "    #                  'energy', 'kin8nm', 'naval', 'power','protein']\n",
    "d_in_list = [28*28]\n",
    "    # , 11, 11, 8, 8, 8, 16, 4, 9]\n",
    "d_out = 10\n",
    "# full Bayesian training settings\n",
    "batch_size = 200\n",
    "lr_0 = 0.1\n",
    "beta = 0.0\n",
    "total_epochs = 2000\n",
    "start_sampling_epoch = 150\n",
    "epochs_per_cycle = 50\n",
    "print_epoch_cycle = 25\n",
    "drop_mixing = 17\n",
    "for i in range(len(dataset_name_list)):\n",
    "    dataset_name = dataset_name_list[i]\n",
    "    hid_n_gp = 30\n",
    "\n",
    "    log_p_coll = []\n",
    "    acc_coll = []\n",
    "    for run_index in range(repeat):\n",
    "        print('*' * 70)\n",
    "        print('*' * 30, '2-layer DGPs ', f'Run {run_index}', '*' * 30)\n",
    "        model = ClassificationDGP(d_in_list[i], d_out, n_hidden_layers=2, n_rf=100,\n",
    "                                  n_gp=[hid_n_gp, d_out], likelihood=Softmax(),\n",
    "                                  kernel_type_list=['RBF', 'RBF'], kernel_trainable=True,\n",
    "                                  random_fixed=True, set_nonzero_mean=False, input_cat=True)\n",
    "        # return matrix [S, N]\n",
    "        log_p, acc = classification_train(model, dataset_name=dataset_name, batch_size=batch_size,\n",
    "                                          lr_0=lr_0, momentum_decay=beta, full_bayesian=True,\n",
    "                                          precond_type='identity',\n",
    "                                          resample_in_cycle_head=False,\n",
    "                                          total_epochs=total_epochs, start_sampling_epoch=start_sampling_epoch,\n",
    "                                          epochs_per_cycle=epochs_per_cycle,\n",
    "                                          print_epoch_cycle=print_epoch_cycle)\n",
    "        log_p_droped = log_p[drop_mixing:, :]\n",
    "        acc_droped = acc[drop_mixing:]\n",
    "\n",
    "        n_models_droped = tf.shape(acc_droped)[0]\n",
    "        predict_log_p = tf.reduce_logsumexp(log_p_droped, axis=0) - tf.math.log(tf.cast(n_models_droped, tf.float32))\n",
    "        predict_log_p = tf.reduce_mean(predict_log_p)\n",
    "        predict_acc = tf.reduce_mean(acc_droped)\n",
    "\n",
    "        print(f\"Dataset: {dataset_name}, Full Bayes \")\n",
    "        print(f\"Number of sampled models(after dropping {drop_mixing} samples): {n_models_droped}\")\n",
    "        print(f\"Test Log Likelihood of all sampled models: {predict_log_p}\")\n",
    "        print(f\"Test Root MSE of all sampled models: {predict_acc}\")\n",
    "\n",
    "        log_p_coll.append(predict_log_p)\n",
    "        acc_coll.append(predict_acc)\n",
    "        print('*' * 70)\n",
    "        print('*' * 70, \"\\n\")\n",
    "\n",
    "    log_p_coll = tf.concat(log_p_coll, axis=0)\n",
    "    acc_coll = tf.concat(acc_coll, axis=0)\n",
    "    with open(record_file, 'a') as f:\n",
    "        f.write(f\"Dataset: {dataset_name}, with no preconditioner \\n\")\n",
    "        f.write(f\"Predict Mean Log Likelihood: {log_p_coll},\\n  \")\n",
    "        f.write(f\"Their mean is: {tf.reduce_mean(log_p_coll)}, \")\n",
    "        f.write(f\"std is: {tf.math.reduce_std(log_p_coll)}\\n\")\n",
    "        f.write(f\"Predict Mean Acc: {acc_coll},\\n  \")\n",
    "        f.write(f\"Their mean is: {tf.reduce_mean(acc_coll)}, \")\n",
    "        f.write(f\"std is: {tf.math.reduce_std(acc_coll)}\\n\\n\")\n",
    "# with RMSProp preconditioner\n",
    "# setting dataset\n",
    "record_file = 'Fashion_MNIST_results.txt'\n",
    "repeat = 2 # run the task several times\n",
    "print('#'*40, f\"Results stored in {record_file}.\", '#'*40)\n",
    "\n",
    "dataset_name_list = ['fashion_mnist']\n",
    "    # , 'wine_red', 'wine_white', 'concrete',\n",
    "    #                  'energy', 'kin8nm', 'naval', 'power','protein']\n",
    "d_in_list = [28*28]\n",
    "    # , 11, 11, 8, 8, 8, 16, 4, 9]\n",
    "d_out = 10\n",
    "# full Bayesian training settings\n",
    "batch_size = 200\n",
    "lr_0 = 0.1\n",
    "beta = 0.0\n",
    "total_epochs = 2000\n",
    "start_sampling_epoch = 150\n",
    "epochs_per_cycle = 50\n",
    "print_epoch_cycle = 25\n",
    "drop_mixing = 17\n",
    "for i in range(len(dataset_name_list)):\n",
    "    dataset_name = dataset_name_list[i]\n",
    "    hid_n_gp = 30\n",
    "\n",
    "    log_p_coll = []\n",
    "    acc_coll = []\n",
    "    for run_index in range(repeat):\n",
    "        print('*' * 70)\n",
    "        print('*' * 30, '2-layer DGPs ', f'Run {run_index}', '*' * 30)\n",
    "        model = ClassificationDGP(d_in_list[i], d_out, n_hidden_layers=2, n_rf=100,\n",
    "                                  n_gp=[hid_n_gp, d_out], likelihood=Softmax(),\n",
    "                                  kernel_type_list=['RBF', 'RBF'], kernel_trainable=True,\n",
    "                                  random_fixed=True, set_nonzero_mean=False, input_cat=True)\n",
    "        # return matrix [S, N]\n",
    "        log_p, acc = classification_train(model, dataset_name=dataset_name, batch_size=batch_size,\n",
    "                                          lr_0=lr_0, momentum_decay=beta, full_bayesian=True,\n",
    "                                          precond_type='rmsprop', K_batches=32,second_moment_centered=False,\n",
    "                                          resample_in_cycle_head=False,\n",
    "                                          total_epochs=total_epochs, start_sampling_epoch=start_sampling_epoch,\n",
    "                                          epochs_per_cycle=epochs_per_cycle,\n",
    "                                          print_epoch_cycle=print_epoch_cycle)\n",
    "        log_p_droped = log_p[drop_mixing:, :]\n",
    "        acc_droped = acc[drop_mixing:]\n",
    "\n",
    "        n_models_droped = tf.shape(acc_droped)[0]\n",
    "        predict_log_p = tf.reduce_logsumexp(log_p_droped, axis=0) - tf.math.log(tf.cast(n_models_droped, tf.float32))\n",
    "        predict_log_p = tf.reduce_mean(predict_log_p)\n",
    "        predict_acc = tf.reduce_mean(acc_droped)\n",
    "\n",
    "        print(f\"Dataset: {dataset_name}, Full Bayes \")\n",
    "        print(f\"Number of sampled models(after dropping {drop_mixing} samples): {n_models_droped}\")\n",
    "        print(f\"Test Log Likelihood of all sampled models: {predict_log_p}\")\n",
    "        print(f\"Test Root MSE of all sampled models: {predict_acc}\")\n",
    "\n",
    "        log_p_coll.append(predict_log_p)\n",
    "        acc_coll.append(predict_acc)\n",
    "        print('*' * 70)\n",
    "        print('*' * 70, \"\\n\")\n",
    "\n",
    "    log_p_coll = tf.concat(log_p_coll, axis=0)\n",
    "    acc_coll = tf.concat(acc_coll, axis=0)\n",
    "    with open(record_file, 'a') as f:\n",
    "        f.write(f\"Dataset: {dataset_name}, with RMSProp preconditioner \\n\")\n",
    "        f.write(f\"Predict Mean Log Likelihood: {log_p_coll},\\n  \")\n",
    "        f.write(f\"Their mean is: {tf.reduce_mean(log_p_coll)}, \")\n",
    "        f.write(f\"std is: {tf.math.reduce_std(log_p_coll)}\\n\")\n",
    "        f.write(f\"Predict Mean Acc: {acc_coll},\\n  \")\n",
    "        f.write(f\"Their mean is: {tf.reduce_mean(acc_coll)}, \")\n",
    "        f.write(f\"std is: {tf.math.reduce_std(acc_coll)}\\n\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}