{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras.optimizers as optimizers\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from models.regression_model import RegressionDGP\n",
    "from likelihoods import Gaussian\n",
    "from utils_dataset import load_UCI_dataset\n",
    "from utils_training import MCEM_sampler, MCEM_Q_maximizer, MCEM, MCEM_windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############################## Getting data info:dataset name: boston ##############################\n",
      "D: 13, N: 455, Ns: 51\n",
      "X_mean: [1.7378345e+00 1.1138461e+01 1.1224440e+01 7.2527476e-02 5.5657738e-01\n",
      " 6.2923098e+00 6.9054504e+01 3.6594815e+00 4.3472528e+00 4.0854724e+02\n",
      " 1.8481098e+01 3.5551254e+02 1.2646418e+01], Y_mean: [22.656263], Y_std: [9.32293]\n",
      "######################################################################\n",
      "Dataset boston, total size:506\n"
     ]
    }
   ],
   "source": [
    "# dataset information loading\n",
    "dataset_name = 'boston'\n",
    "_, _, train_shape, test_shape = load_UCI_dataset(dataset_name, data_dir='./data/')\n",
    "dataset_size = train_shape[0] + test_shape[0]\n",
    "print(f\"Dataset {dataset_name}, total size:{dataset_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2-layer DGPs with Kernel type RBF\n"
     ]
    }
   ],
   "source": [
    "# model setting\n",
    "d_in = train_shape[1]\n",
    "d_out = 1\n",
    "n_gp = 10\n",
    "model = RegressionDGP(d_in, d_out, n_hidden_layers=2, n_rf=100, n_gp=n_gp,\n",
    "                      likelihood=Gaussian(variance=0.1, trainable=True),\n",
    "                      kernel_type_list=['RBF','RBF'], kernel_trainable=True,\n",
    "                      random_fixed=True, input_cat=True)\n",
    "print(f\"{model.n_hidden_layers}-layer DGPs with Kernel type {model.kernel_type_list[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############################## Getting data info:dataset name: boston ##############################\n",
      "D: 13, N: 455, Ns: 51\n",
      "X_mean: [1.7378345e+00 1.1138461e+01 1.1224440e+01 7.2527476e-02 5.5657738e-01\n",
      " 6.2923098e+00 6.9054504e+01 3.6594815e+00 4.3472528e+00 4.0854724e+02\n",
      " 1.8481098e+01 3.5551254e+02 1.2646418e+01], Y_mean: [22.656263], Y_std: [9.32293]\n",
      "######################################################################\n",
      "Training size is 400 after remainder dropping. \n",
      "############################## Getting data info:dataset name: boston ##############################\n",
      "D: 13, N: 455, Ns: 51\n",
      "X_mean: [1.7378345e+00 1.1138461e+01 1.1224440e+01 7.2527476e-02 5.5657738e-01\n",
      " 6.2923098e+00 6.9054504e+01 3.6594815e+00 4.3472528e+00 4.0854724e+02\n",
      " 1.8481098e+01 3.5551254e+02 1.2646418e+01], Y_mean: [22.656263], Y_std: [9.32293]\n",
      "######################################################################\n",
      "Training size is 400 after remainder dropping. \n"
     ]
    }
   ],
   "source": [
    "# EM sampler settings\n",
    "batch_size=200\n",
    "lr_mcmc_0 = 0.01\n",
    "beta_mcmc = 0.9\n",
    "sampler_EM = MCEM_sampler(model, dataset_name=dataset_name, batch_size=batch_size, data_dir='./data/',\n",
    "                          lr_0=lr_mcmc_0, momentum_decay=beta_mcmc,\n",
    "                          precond_type='identity', K_batches=None, second_moment_centered=None,\n",
    "                          resample_in_cycle_head=False, start_sampling_epoch=2000, epochs_per_cycle=50)\n",
    "# Maximizer setttings\n",
    "lr_maximizer = 0.01\n",
    "optimizer = optimizers.Adam(learning_rate=lr_maximizer)\n",
    "maximizer = MCEM_Q_maximizer(model, dataset_size, optimizer)\n",
    "# sampler settings after fixing hyper-params\n",
    "lr_fixing_hyper_0 = 0.01\n",
    "beta_fixing = 0.9\n",
    "sampler_fixing_hyper = MCEM_sampler(model, dataset_name=dataset_name, batch_size=batch_size, data_dir='./data/',\n",
    "                                    lr_0=lr_fixing_hyper_0, momentum_decay=beta_fixing,\n",
    "                                    precond_type='identity', K_batches=None, second_moment_centered=None,\n",
    "                                    resample_in_cycle_head=False, start_sampling_epoch=2000, epochs_per_cycle=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# MCEM settings and training\n",
    "total_EM_steps = 20000\n",
    "ds_train_M, _, _, _ = load_UCI_dataset(dataset_name, batch_size=100, data_dir='./data/')\n",
    "log_p, mse = MCEM(sampler_EM, maximizer, sampler_fixing_hyper, total_EM_steps, ds_train_M,\n",
    "                      num_samples_EM=100, num_samples_fixing_hyper=200,\n",
    "                      print_epoch_cycle_EM=200, print_epoch_cycle_fixing=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "log_p_droped = log_p[drop_mixing:, :]\n",
    "mse_droped = mse[drop_mixing:, :]\n",
    "\n",
    "n_models_droped = tf.shape(mse_droped)[0]\n",
    "predict_log_p = tf.reduce_logsumexp(log_p_droped, axis=0) - tf.math.log(tf.cast(n_models_droped, tf.float32))\n",
    "predict_log_p = tf.reduce_mean(predict_log_p)\n",
    "predict_rmse = tf.math.sqrt(tf.reduce_mean(mse_droped))\n",
    "\n",
    "print(f\"Dataset: {dataset_name}\")\n",
    "print(f\"Number of sampled models(after dropping {drop_mixing} samples): {n_models_droped}\")\n",
    "print(f\"Test Log Likelihood of all sampled models: {predict_log_p}\")\n",
    "print(f\"Test Root MSE of all sampled models: {predict_rmse}\")\n",
    "\n",
    "log_p_coll.append(predict_log_p)\n",
    "rmse_coll.append(predict_rmse)\n",
    "print('*' * 70)\n",
    "print('*' * 70)\n",
    "print(' ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "record_file = 'train_regression_results_2.txt'\n",
    "repeat = 3 # run the task several times\n",
    "print('#'*40, f\"Results stored in {record_file}.\", '#'*40)\n",
    "\n",
    "dataset_name_list = ['boston', 'wine_red', 'wine_white', 'concrete',\n",
    "                     'energy', 'kin8nm', 'naval', 'power','protein']\n",
    "d_in_list = [13, 11, 11, 8, 8, 8, 16, 4, 9]\n",
    "d_out = 1\n",
    "batch_size = 200\n",
    "lr_0 = 0.01\n",
    "beta = 0.99\n",
    "total_epochs = 50000\n",
    "start_sampling_epoch = 40000\n",
    "epochs_per_cycle = 100\n",
    "print_epoch_cycle = 200\n",
    "drop_mixing = 50\n",
    "\n",
    "for i in range(len(dataset_name_list)):\n",
    "    dataset_name = dataset_name_list[i]\n",
    "    hid_n_gp = min(d_in_list[i], 30)\n",
    "\n",
    "    log_p_coll = []\n",
    "    rmse_coll = []\n",
    "    for run_index in range(repeat):\n",
    "        print('*' * 70)\n",
    "        print('*' * 30, '2-layer DGPs ', f'Run {run_index}', '*' * 30)\n",
    "        model = RegressionDGP(d_in_list[i], d_out, n_hidden_layers=2, n_rf=300,\n",
    "                              n_gp=[hid_n_gp, d_out], likelihood=Gaussian(),\n",
    "                              kernel_type_list=['RBF', 'RBF'], kernel_trainable=True,\n",
    "                              random_fixed=True, set_nonzero_mean=False, input_cat=True)\n",
    "        # return matrix [S, N]\n",
    "        log_p, mse = regression_train(model, dataset_name=dataset_name, batch_size=batch_size,\n",
    "                                      lr_0=lr_0, momentum_decay=beta,\n",
    "                                      precond_type='rmsprop', K_batches=5, second_moment_centered=False,\n",
    "                                      resample_in_cycle_head=False,\n",
    "                                      total_epochs=total_epochs, start_sampling_epoch=start_sampling_epoch,\n",
    "                                      epochs_per_cycle=epochs_per_cycle,\n",
    "                                      print_epoch_cycle=print_epoch_cycle)\n",
    "        log_p_droped = log_p[drop_mixing:, :]\n",
    "        mse_droped = mse[drop_mixing:, :]\n",
    "\n",
    "        n_models_droped = tf.shape(mse_droped)[0]\n",
    "        predict_log_p = tf.reduce_logsumexp(log_p_droped, axis=0) - tf.math.log(tf.cast(n_models_droped, tf.float32))\n",
    "        predict_log_p = tf.reduce_mean(predict_log_p)\n",
    "        predict_rmse = tf.math.sqrt(tf.reduce_mean(mse_droped))\n",
    "\n",
    "        print(f\"Dataset: {dataset_name}\")\n",
    "        print(f\"Number of sampled models(after dropping {drop_mixing} samples): {n_models_droped}\")\n",
    "        print(f\"Test Log Likelihood of all sampled models: {predict_log_p}\")\n",
    "        print(f\"Test Root MSE of all sampled models: {predict_rmse}\")\n",
    "\n",
    "        log_p_coll.append(predict_log_p)\n",
    "        rmse_coll.append(predict_rmse)\n",
    "        print('*' * 70)\n",
    "        print('*' * 70)\n",
    "        print(' ')\n",
    "\n",
    "    log_p_coll = tf.concat(log_p_coll, axis=0)\n",
    "    rmse_coll = tf.concat(rmse_coll, axis=0)\n",
    "    with open(record_file, 'a') as f:\n",
    "        f.write(f\"Dataset: {dataset_name}, \\n\")\n",
    "        f.write(f\"Predict Mean Log Likelihood: {log_p_coll},\\n  \")\n",
    "        f.write(f\"Their mean is: {tf.reduce_mean(log_p_coll)}, \")\n",
    "        f.write(f\"std is: {tf.math.reduce_std(log_p_coll)}\\n\")\n",
    "        f.write(f\"Predict RMSE: {rmse_coll},\\n  \")\n",
    "        f.write(f\"Their mean is: {tf.reduce_mean(rmse_coll)}, \")\n",
    "        f.write(f\"std is: {tf.math.reduce_std(rmse_coll)}\\n\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
